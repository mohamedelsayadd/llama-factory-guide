{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset"
      ],
      "metadata": {
        "id": "gUamY-vOxoA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRdLJ0USBdBT",
        "outputId": "88f3b679-58fb-4faa-ea63-320992038c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['output', 'input', 'instruction'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"yahma/alpaca-cleaned\",split=\"train\").select(range(1000))\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can use huggingface datasets or any other dataset type, even your custom data"
      ],
      "metadata": {
        "id": "F0j2MogGxylW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnSdDdANDwwO",
        "outputId": "d675d5aa-6fba-45d1-8a1a-a63a2d4a6be2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n",
              " 'input': '',\n",
              " 'instruction': 'Give three tips for staying healthy.'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model"
      ],
      "metadata": {
        "id": "m9J2E-TmyNAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"what is the capital city of Egypt?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Toy260BxEhsO",
        "outputId": "bb482b45-78cf-417e-8586-599d395cd23b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            " ÿßŸÑÿ¨ÿ≤ÿßÿ¶\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can use instruction tuning model or base model"
      ],
      "metadata": {
        "id": "DclGYVvxyXTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install llama-factory"
      ],
      "metadata": {
        "id": "JQnWhtwlyRs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "!pip install -e \".[torch,metrics]\" --no-build-isolation"
      ],
      "metadata": {
        "id": "CcC7c2o0JLQC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Give access to wandb to monitor the traning process"
      ],
      "metadata": {
        "id": "1Zsmxnxsymzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "wandb.login(key=userdata.get('w&b_token'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2SsXQP2JcJ0",
        "outputId": "0f1d2f9f-b872-411c-c99a-c36c581e5ead"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamedelsayad866\u001b[0m (\u001b[33mmohamedelsayad866-met\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Data in json file"
      ],
      "metadata": {
        "id": "ZAHMitD3zBqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the data must be in the following Structure:"
      ],
      "metadata": {
        "id": "kWwhD-WXzMEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```json\n",
        "{\n",
        "\"dataset_name\": {\n",
        "  \"file_name\": \"data.json\",\n",
        "  \"columns\": {\n",
        "    \"prompt\": \"instruction\",\n",
        "    \"query\": \"input\",\n",
        "    \"response\": \"output\",\n",
        "    \"system\": \"system\",\n",
        "    \"history\": \"history\"\n",
        "  }\n",
        " }\n",
        "}"
      ],
      "metadata": {
        "id": "ICAFwIRdzXv8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ee19e60",
        "outputId": "128e7e61-df48-41af-ebc1-e0b03ade1ab2"
      },
      "source": [
        "import json\n",
        "\n",
        "data_list = ds.to_list()\n",
        "\n",
        "json_filename = \"/content/drive/MyDrive/dataset.json\"\n",
        "\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump(data_list, f, indent=4)\n",
        "\n",
        "print(f\"Dataset saved to {json_filename}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved to /content/drive/MyDrive/dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**now you have to open the following file and add your dataset in the following format:**"
      ],
      "metadata": {
        "id": "oqw1lxN70MhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLaMA-Factory/data/dataset_info.json"
      ],
      "metadata": {
        "id": "ee0a7LUr1dAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```json\n",
        "{\n",
        "  \"dataset_name\": {\n",
        "  \"file_name\": \"data.json\",\n",
        "  \"ranking\": true,\n",
        "  \"columns\": {\n",
        "    \"prompt\": \"instruction\",\n",
        "    \"query\": \"input\",\n",
        "    \"chosen\": \"chosen\",\n",
        "    \"rejected\": \"rejected\"\n",
        "  }\n",
        " }\n",
        "}"
      ],
      "metadata": {
        "id": "GgvG1OMe0xV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add the configration file to the following path:"
      ],
      "metadata": {
        "id": "yf4Ftz631Apj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/LLaMA-Factory/examples/train_lora/"
      ],
      "metadata": {
        "id": "57ASLBvT1POT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edit the following configration as you want"
      ],
      "metadata": {
        "id": "Kmqb0UxE2FDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/alpaca-cleaned.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: alpaca-cleaned\n",
        "template: qwen\n",
        "cutoff_len: 1024\n",
        "# max_samples: 1000\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "dataloader_num_workers: 4\n",
        "\n",
        "### output\n",
        "output_dir: saves/qwen/lora/sft\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "overwrite_output_dir: true\n",
        "save_only_model: false\n",
        "report_to: wandb  # choices: [none, wandb, tensorboard, swanlab, mlflow]\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 8\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "resume_from_checkpoint: null\n",
        "\n",
        "# eval\n",
        "# eval_dataset: alpaca_en_demo\n",
        "val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 500\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0HQjGo7MNJ1",
        "outputId": "79954424-6ccc-45d7-b5f3-d4a927ba07c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/LLaMA-Factory/examples/train_lora/alpaca-cleaned.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally use this command to train"
      ],
      "metadata": {
        "id": "-Qmrs-K62T5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "replace the yaml file path with your own"
      ],
      "metadata": {
        "id": "PIFOtd642bYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/LLaMA-Factory && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/alpaca-cleaned.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfDhgzG2NsOr",
        "outputId": "a2ede5a8-2b28-4b3f-cb02-a6bfad0b2552"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-20 13:05:46.957871: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758373546.978953   12718 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758373546.986099   12718 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758373547.001905   12718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758373547.001933   12718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758373547.001937   12718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758373547.001940   12718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[INFO|2025-09-20 13:05:59] llamafactory.hparams.parser:414 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:05:59,483 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:05:59,483 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:05:59,483 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:05:59,483 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:05:59,483 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:05:59,483 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:05:59,483 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2337] 2025-09-20 13:05:59,816 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:765] 2025-09-20 13:06:00,483 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-09-20 13:06:00,485 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:06:00,698 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:06:00,698 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:06:00,698 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:06:00,698 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:06:00,698 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:06:00,698 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2068] 2025-09-20 13:06:00,698 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2337] 2025-09-20 13:06:01,022 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-09-20 13:06:01] llamafactory.data.template:143 >> Replace eos token: <|im_end|>.\n",
            "[INFO|2025-09-20 13:06:01] llamafactory.data.loader:143 >> Loading dataset /content/drive/MyDrive/dataset.json...\n",
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "Converting format of dataset (num_proc=16): 2000 examples [00:01, 604.10 examples/s] \n",
            "Running tokenizer on dataset (num_proc=16): 2000 examples [00:12, 82.47 examples/s] \n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 35127, 2326, 10414, 369, 19429, 9314, 13, 151645, 198, 151644, 77091, 198, 16, 13, 44514, 264, 23831, 323, 77116, 9968, 25, 7405, 2704, 697, 20969, 525, 28308, 315, 264, 8045, 315, 25322, 323, 23880, 11, 15651, 12833, 11, 4361, 40836, 11, 323, 9314, 49027, 13, 1096, 8609, 311, 3410, 697, 2487, 448, 279, 7565, 36393, 311, 729, 518, 1181, 1850, 323, 646, 1492, 5358, 20601, 18808, 382, 17, 13, 3285, 424, 304, 5792, 6961, 5702, 25, 32818, 374, 16587, 369, 20337, 3746, 24854, 11, 23648, 11, 323, 40613, 2820, 13, 70615, 369, 518, 3245, 220, 16, 20, 15, 4420, 315, 23193, 90390, 10158, 476, 220, 22, 20, 4420, 315, 70820, 10158, 1817, 2003, 382, 18, 13, 2126, 3322, 6084, 25, 24515, 3322, 4271, 6084, 374, 16587, 369, 6961, 323, 10502, 1632, 32751, 13, 1084, 8609, 311, 36277, 19671, 11, 7269, 24675, 729, 11, 323, 11554, 9314, 6513, 323, 22077, 729, 13, 70615, 369, 220, 22, 12, 24, 4115, 315, 6084, 1817, 3729, 13, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Give three tips for staying healthy.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 44514, 264, 23831, 323, 77116, 9968, 25, 7405, 2704, 697, 20969, 525, 28308, 315, 264, 8045, 315, 25322, 323, 23880, 11, 15651, 12833, 11, 4361, 40836, 11, 323, 9314, 49027, 13, 1096, 8609, 311, 3410, 697, 2487, 448, 279, 7565, 36393, 311, 729, 518, 1181, 1850, 323, 646, 1492, 5358, 20601, 18808, 382, 17, 13, 3285, 424, 304, 5792, 6961, 5702, 25, 32818, 374, 16587, 369, 20337, 3746, 24854, 11, 23648, 11, 323, 40613, 2820, 13, 70615, 369, 518, 3245, 220, 16, 20, 15, 4420, 315, 23193, 90390, 10158, 476, 220, 22, 20, 4420, 315, 70820, 10158, 1817, 2003, 382, 18, 13, 2126, 3322, 6084, 25, 24515, 3322, 4271, 6084, 374, 16587, 369, 6961, 323, 10502, 1632, 32751, 13, 1084, 8609, 311, 36277, 19671, 11, 7269, 24675, 729, 11, 323, 11554, 9314, 6513, 323, 22077, 729, 13, 70615, 369, 220, 22, 12, 24, 4115, 315, 6084, 1817, 3729, 13, 151645, 198]\n",
            "labels:\n",
            "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
            "\n",
            "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
            "\n",
            "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:765] 2025-09-20 13:06:15,525 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-09-20 13:06:15,526 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-09-20 13:06:15] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[WARNING|logging.py:328] 2025-09-20 13:06:16,078 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO|modeling_utils.py:1280] 2025-09-20 13:06:16,078 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/model.safetensors\n",
            "[INFO|modeling_utils.py:2466] 2025-09-20 13:06:16,079 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1055] 2025-09-20 13:06:16,081 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5721] 2025-09-20 13:06:28,791 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5729] 2025-09-20 13:06:28,791 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1010] 2025-09-20 13:06:28,915 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/generation_config.json\n",
            "[INFO|configuration_utils.py:1055] 2025-09-20 13:06:28,915 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"max_new_tokens\": 2048\n",
            "}\n",
            "\n",
            "[INFO|2025-09-20 13:06:29] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-09-20 13:06:29] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-09-20 13:06:29] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-09-20 13:06:29] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-09-20 13:06:29] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,gate_proj,q_proj,up_proj,v_proj,down_proj,k_proj\n",
            "[INFO|2025-09-20 13:06:30] llamafactory.model.loader:143 >> trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n",
            "[INFO|trainer.py:757] 2025-09-20 13:06:30,706 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:985] 2025-09-20 13:06:30,708 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:2523] 2025-09-20 13:06:31,586 >> ***** Running training *****\n",
            "[INFO|trainer.py:2524] 2025-09-20 13:06:31,586 >>   Num examples = 900\n",
            "[INFO|trainer.py:2525] 2025-09-20 13:06:31,586 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2526] 2025-09-20 13:06:31,587 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2529] 2025-09-20 13:06:31,587 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2530] 2025-09-20 13:06:31,587 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2531] 2025-09-20 13:06:31,587 >>   Total optimization steps = 339\n",
            "[INFO|trainer.py:2532] 2025-09-20 13:06:31,595 >>   Number of trainable parameters = 73,859,072\n",
            "[INFO|integration_utils.py:869] 2025-09-20 13:06:31,601 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamedelsayad866\u001b[0m (\u001b[33mmohamedelsayad866-met\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m creating run (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250920_130631-bplp2wd4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglowing-valley-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/mohamedelsayad866-met/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mohamedelsayad866-met/llamafactory/runs/bplp2wd4\u001b[0m\n",
            "  0% 0/339 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "{'loss': 1.2758, 'grad_norm': 1.4976255893707275, 'learning_rate': 2.647058823529412e-05, 'epoch': 0.09}\n",
            "{'loss': 1.1129, 'grad_norm': 1.003691554069519, 'learning_rate': 5.588235294117647e-05, 'epoch': 0.18}\n",
            "{'loss': 1.191, 'grad_norm': 1.1098624467849731, 'learning_rate': 8.529411764705883e-05, 'epoch': 0.27}\n",
            "{'loss': 1.1527, 'grad_norm': 0.7417737245559692, 'learning_rate': 9.993370449424153e-05, 'epoch': 0.36}\n",
            "{'loss': 1.1007, 'grad_norm': 0.7924438714981079, 'learning_rate': 9.940439480455386e-05, 'epoch': 0.44}\n",
            "{'loss': 1.089, 'grad_norm': 5.670668125152588, 'learning_rate': 9.835138623956603e-05, 'epoch': 0.53}\n",
            "{'loss': 1.1362, 'grad_norm': 0.5778041481971741, 'learning_rate': 9.678584095202468e-05, 'epoch': 0.62}\n",
            "{'loss': 1.195, 'grad_norm': 0.6797682046890259, 'learning_rate': 9.472435411143978e-05, 'epoch': 0.71}\n",
            "{'loss': 1.1208, 'grad_norm': 0.8409643173217773, 'learning_rate': 9.218877799115928e-05, 'epoch': 0.8}\n",
            "{'loss': 1.1094, 'grad_norm': 0.995521068572998, 'learning_rate': 8.920599032883554e-05, 'epoch': 0.89}\n",
            "{'loss': 1.2726, 'grad_norm': 0.9932613372802734, 'learning_rate': 8.580760941571967e-05, 'epoch': 0.98}\n",
            "{'loss': 0.9272, 'grad_norm': 0.9875799417495728, 'learning_rate': 8.202965893490878e-05, 'epoch': 1.06}\n",
            "{'loss': 0.8557, 'grad_norm': 0.6470884680747986, 'learning_rate': 7.791218610134323e-05, 'epoch': 1.15}\n",
            "{'loss': 0.9367, 'grad_norm': 0.993259847164154, 'learning_rate': 7.3498837151366e-05, 'epoch': 1.24}\n",
            "{'loss': 0.8355, 'grad_norm': 0.9115724563598633, 'learning_rate': 6.883639468175927e-05, 'epoch': 1.33}\n",
            "{'loss': 0.8838, 'grad_norm': 0.8021672368049622, 'learning_rate': 6.397428174258047e-05, 'epoch': 1.42}\n",
            "{'loss': 0.8439, 'grad_norm': 1.3748958110809326, 'learning_rate': 5.896403794053679e-05, 'epoch': 1.51}\n",
            "{'loss': 0.997, 'grad_norm': 1.0385700464248657, 'learning_rate': 5.385877310633233e-05, 'epoch': 1.6}\n",
            "{'loss': 0.8314, 'grad_norm': 1.1316554546356201, 'learning_rate': 4.8712604317250576e-05, 'epoch': 1.68}\n",
            "{'loss': 0.8697, 'grad_norm': 1.1605859994888306, 'learning_rate': 4.358008224267245e-05, 'epoch': 1.77}\n",
            "{'loss': 0.7899, 'grad_norm': 1.1506870985031128, 'learning_rate': 3.851561289341023e-05, 'epoch': 1.86}\n",
            "{'loss': 0.7872, 'grad_norm': 1.1596606969833374, 'learning_rate': 3.357288090445827e-05, 'epoch': 1.95}\n",
            "{'loss': 0.7959, 'grad_norm': 0.8993321061134338, 'learning_rate': 2.8804280464506973e-05, 'epoch': 2.04}\n",
            "{'loss': 0.6888, 'grad_norm': 1.0655722618103027, 'learning_rate': 2.426035992450848e-05, 'epoch': 2.12}\n",
            "{'loss': 0.5418, 'grad_norm': 1.0757184028625488, 'learning_rate': 1.9989285972581595e-05, 'epoch': 2.21}\n",
            "{'loss': 0.6713, 'grad_norm': 1.0884298086166382, 'learning_rate': 1.6036333055135344e-05, 'epoch': 2.3}\n",
            "{'loss': 0.5281, 'grad_norm': 1.7068817615509033, 'learning_rate': 1.2443403456474017e-05, 'epoch': 2.39}\n",
            "{'loss': 0.6299, 'grad_norm': 1.2884153127670288, 'learning_rate': 9.248583124159438e-06, 'epoch': 2.48}\n",
            "{'loss': 0.6367, 'grad_norm': 1.384453296661377, 'learning_rate': 6.4857379484922375e-06, 'epoch': 2.57}\n",
            "{'loss': 0.5561, 'grad_norm': 1.041238784790039, 'learning_rate': 4.184154775649768e-06, 'epoch': 2.66}\n",
            "{'loss': 0.5549, 'grad_norm': 1.1793726682662964, 'learning_rate': 2.3682309598308747e-06, 'epoch': 2.75}\n",
            "{'loss': 0.5895, 'grad_norm': 1.1647692918777466, 'learning_rate': 1.0572157452321097e-06, 'epoch': 2.84}\n",
            "{'loss': 0.5818, 'grad_norm': 1.520023226737976, 'learning_rate': 2.6500621927054715e-07, 'epoch': 2.92}\n",
            "100% 339/339 [54:35<00:00,  7.68s/it][INFO|trainer.py:4289] 2025-09-20 14:01:09,393 >> Saving model checkpoint to saves/llama3-8b/lora/sft/checkpoint-339\n",
            "[INFO|configuration_utils.py:765] 2025-09-20 14:01:09,677 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-09-20 14:01:09,678 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2394] 2025-09-20 14:01:46,438 >> chat template saved in saves/llama3-8b/lora/sft/checkpoint-339/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2563] 2025-09-20 14:01:46,440 >> tokenizer config file saved in saves/llama3-8b/lora/sft/checkpoint-339/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2572] 2025-09-20 14:01:46,440 >> Special tokens file saved in saves/llama3-8b/lora/sft/checkpoint-339/special_tokens_map.json\n",
            "[INFO|trainer.py:2808] 2025-09-20 14:02:23,456 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3351.8623, 'train_samples_per_second': 0.806, 'train_steps_per_second': 0.101, 'train_loss': 0.8730593585686698, 'epoch': 3.0}\n",
            "100% 339/339 [55:49<00:00,  9.88s/it]\n",
            "[INFO|trainer.py:4289] 2025-09-20 14:02:23,462 >> Saving model checkpoint to saves/llama3-8b/lora/sft\n",
            "[INFO|configuration_utils.py:765] 2025-09-20 14:02:23,757 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/config.json\n",
            "[INFO|configuration_utils.py:839] 2025-09-20 14:02:23,759 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2394] 2025-09-20 14:02:39,699 >> chat template saved in saves/llama3-8b/lora/sft/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2563] 2025-09-20 14:02:39,700 >> tokenizer config file saved in saves/llama3-8b/lora/sft/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2572] 2025-09-20 14:02:39,701 >> Special tokens file saved in saves/llama3-8b/lora/sft/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  3849526GF\n",
            "  train_loss               =     0.8731\n",
            "  train_runtime            = 0:55:51.86\n",
            "  train_samples_per_second =      0.806\n",
            "  train_steps_per_second   =      0.101\n",
            "Figure saved at: saves/llama3-8b/lora/sft/training_loss.png\n",
            "[WARNING|2025-09-20 14:02:45] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-09-20 14:02:45] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:4623] 2025-09-20 14:02:45,296 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4625] 2025-09-20 14:02:45,296 >>   Num examples = 100\n",
            "[INFO|trainer.py:4628] 2025-09-20 14:02:45,296 >>   Batch size = 1\n",
            "100% 100/100 [00:41<00:00,  2.39it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_loss               =      1.432\n",
            "  eval_runtime            = 0:00:42.84\n",
            "  eval_samples_per_second =      2.334\n",
            "  eval_steps_per_second   =      2.334\n",
            "[INFO|modelcard.py:456] 2025-09-20 14:03:28,149 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mglowing-valley-5\u001b[0m at: \u001b[34mhttps://wandb.ai/mohamedelsayad866-met/llamafactory/runs/bplp2wd4\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250920_130631-bplp2wd4/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the adapter from the path you selected in the yaml file"
      ],
      "metadata": {
        "id": "95pGiz5X2lmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_adapter(\"saves/qwen/lora/sft\")"
      ],
      "metadata": {
        "id": "GH32fVgGcK3V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model after fine tuning"
      ],
      "metadata": {
        "id": "bqe9TZGI2zTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"what is the capital city of Egypt?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md_inA0scPcS",
        "outputId": "78adb033-af11-4edb-da8e-d5c56b7fdecc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital city of Egypt is Cairo. It is located in the Nile Delta region and is the largest city in the country, with a population of over 16 million people. Cairo is also the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**That's It!**"
      ],
      "metadata": {
        "id": "KezXsu4ZcgxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eng. Mohamed Elsayad"
      ],
      "metadata": {
        "id": "tMXMZgBp28RT"
      }
    }
  ]
}